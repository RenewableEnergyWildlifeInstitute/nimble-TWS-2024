---
title: "Bayesian variable selection"
subtitle: "TWS 2024 Workshop"
author: "NIMBLE Development Team"
date: "October 2024"
output:
  slidy_presentation: default
  beamer_presentation: default
---

<style>
slides > slide {
  overflow-x: auto !important;
  overflow-y: auto !important;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, dvi=36)
library(nimble)
library(compareMCMCs)
recalculate <- TRUE
```

Bayesian variable selection
=====

- You have many candidate explanatory variables.
- Bayesian approach is to have a probability that a variable is included in the model.
- Really this is a probability that the coefficient is $\ne 0$.
- Common implementation is with indicator variables.
- This has problems.  Let's look at it.

Set up a model
=====
Use linear regression for simplicity.

- 5 "real" effects (true coefficient $\ne 0$)
- 10 null effects  (true coefficient $= 0$).

### Create `nimble` model
```{r}
library(nimble)
lmCode <- nimbleCode({
  psi ~ dunif(0,1)   # prior on inclusion probability
  sigma ~ dunif(0, 20)
  for(i in 1:numVars) {
    z[i] ~ dbern(psi) # indicator variable
    beta[i] ~ dnorm(0, sd = 100)
    zbeta[i] <- z[i] * beta[i]  # indicator * beta
  }
  for(i in 1:n) {
    pred.y[i] <- inprod(X[i, 1:numVars], zbeta[1:numVars])
    y[i] ~ dnorm(pred.y[i], sd = sigma)
  }
})
set.seed(1)
X <- matrix(rnorm(100*15), nrow = 100, ncol = 15)
lmConstants <- list(numVars = 15, n = 100, X = X)
lmModel <- nimbleModel(lmCode, constants = lmConstants)
```

### Simulate data using `nimble` model.
```{r}
true_betas <- c(c(0.1, 0.2, 0.3, 0.4, 0.5),
                rep(0, 10))
lmModel$beta <- true_betas
lmModel$sigma <- 1
lmModel$z <- rep(1, 15)
lmModel$psi <- 0.5
lmModel$calculate()
set.seed(0) ## Make this reproducible
lmModel$simulate('y')
lmModel$y
lmModel$calculate() 
lmModel$setData('y')
lmData = list(y = lmModel$y)
```

Look at `lm`
=====
It will be helpful to refer to back simple linear regression:
```{r}
summary(lm(lmModel$y ~ lmModel$X))
```

Look at nimble results with default samplers.
=====
```{r}
MCMCconf <- configureMCMC(lmModel)
MCMCconf$addMonitors('z')
MCMC <- buildMCMC(MCMCconf)
ClmModel <- compileNimble(lmModel)
CMCMC <- compileNimble(MCMC, project = lmModel, resetFunctions = TRUE)
set.seed(100)
system.time(samples_nimble <- runMCMC(CMCMC, niter = 100000, nburnin = 10000))
```


Look at default nimble results
=====

### Look at beta[1]
```{r}
inds <- seq(50000, 60000, by=10) ## Look at arbitrary 1001 iterations thinned
plot(samples_nimble[inds,'beta[1]'])
plot(samples_nimble[inds,'z[1]'])
```

### Look at beta[4]
```{r}
plot(samples_nimble[inds,'beta[4]'], ylim = c(-1, 1))
plot(samples_nimble[inds,'z[4]'])
```

### Look at beta[5]
```{r}
plot(samples_nimble[inds,'beta[5]'])
plot(samples_nimble[inds,'z[5]'])
```

### Look at posterior inclusion probabilities from each `beta[i]`
```{r}
zCols <- grep("z\\[", colnames(samples_nimble))
posterior_inclusion_prob_nimble <- colMeans(samples_nimble[,zCols])
plot(true_betas, posterior_inclusion_prob_nimble)
```

### Look at posterior inclusion probability, `psi`
```{r}
plot(density(samples_nimble[,'psi']))
```

Summary of results from default nimble
=====

- `beta[3]`, `beta[4]` and `beta[5]` are often included.
- When `z[i] = 0`, the corresponding `beta[i]` will start following its **prior**.  It may wander far away from reasonable values, with either fast or slow mixing (depending on the situation).
- Different samplers (e.g. slice samplers for `beta[i]`s) could do better but not solve the fundamental problem.

Summary of Mixing problems (the main issue)
=====

- The model doesn't understand the role of `z[i]`.
- When `z[i] = 0`, the corresponding `beta[i]` will start following its **prior**.
- A proposal to set `z[i] = 1` can only be accepted if `beta[i]` has a reasonable value.
- This creates poor mixing over `z[i]`s.

     - With a slice sampler (e.g. JAGS default) `beta[i]` mixes better over its prior, and `z[i]` doesn't get set to 0 unless `beta[i]` happens to hit a small range of values to be included in the model.
     - With a random-walk MH sampler (default nimble), adaptation depends on `z[i]` (is it adapting to prior or posterior?), and this affects mixing when `z[i]` is 0.  Behavior seems to be problematic in this example.
          
- Conventional solution in BUGS/JAGS language: Use informative prior for `beta[i]` to avoid these problems.  This amounts to changing the model because the MCMC implementation has a problem, and that is never ideal.

### Wasteful computation (a secondary issue)
- When `z[i] = 0`, we'd like to not be wasting computation on `beta[i]`.

Solution: Reversible Jump MCMC
=====

- RJMCMC is a method for sampling across different models.
- Specifically it is about sampling between different numbers of dimensions.
- We don't change the actual nimble model object, but we turn on and off which dimensions are sampled.
- Implementation, like all samplers, is written using `nimbleFunction`s.

RJMCMC for variable selection in nimble
=====

- Update an MCMC configuration to use RJMCMC.

```{r}
# make a new copy of the model to be totally independent
lmModel2 <- lmModel$newModel(replicate = TRUE)
MCMCconfRJ <- configureMCMC(lmModel2)
MCMCconfRJ$addMonitors('z')
configureRJ(MCMCconfRJ,
            targetNodes = 'beta',
            indicatorNodes = 'z',
            control = list(mean = 0, scale = .2))
MCMCRJ <- buildMCMC(MCMCconfRJ)
```

Run the RJMCMC
=====
```{r}
ClmModel2 <- compileNimble(lmModel2)
CMCMCRJ <- compileNimble(MCMCRJ, project = lmModel2)
set.seed(100)
system.time(samples_nimble_RJ <- runMCMC(CMCMCRJ, niter = 100000, nburnin = 10000))
```

Look at RJMCMC results
=====

### Look at beta[1] 
```{r}
inds <- seq(50000, 60000, by = 10)
plot(samples_nimble_RJ[inds,'beta[1]'])
plot(samples_nimble_RJ[inds,'z[1]'])
```

### Look at beta[4]
```{r}
plot(samples_nimble_RJ[inds,'beta[4]'])
plot(samples_nimble_RJ[inds,'z[4]'])
```

### Look at beta[5]
```{r}
plot(samples_nimble_RJ[inds,'beta[5]'])
plot(samples_nimble_RJ[inds,'z[5]'])
```

### Look at posterior inclusion probabilities of each `beta[i]`
```{r}
zCols <- grep("z\\[", colnames(samples_nimble_RJ))
posterior_inclusion_prob_nimble_RJ <- colMeans(samples_nimble_RJ[,zCols])
plot(true_betas, posterior_inclusion_prob_nimble_RJ)
```

### Look at posterior inclusion probability, `psi`
```{r}
plot(density(samples_nimble_RJ[,'psi']))
```

Summary of RJMCMC
=====

- Mixing was much better.
- Adaptation for coefficient samplers only occurs when the coefficient is "in the model".
- Run time was much faster than default nimble, which was faster than JAGS.  Magnitudes will depend on specific problems (how often `z[i]` are 0.)
- Tuning parameter of RJ proposal scale (sd) must be chosen.


